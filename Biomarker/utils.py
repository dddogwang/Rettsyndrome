import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import statsmodels.stats.multitest as smm
from scipy.stats import ttest_ind
from scipy.stats import mannwhitneyu


# åˆ é™¤ç¦»ç¾¤ç‚¹
def remove_outliers(df, multiplier=1.5):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    return df[~((df < (Q1 - multiplier * IQR)) | (df > (Q3 + multiplier * IQR))).any(axis=1)]


def loadcsv(target, stain_type, rett_type):
    if target=="features_image":
        loadpath_RETT = f"tables/{target}/features_{rett_type}_RETT_{stain_type}.csv"
        loadpath_CTRL = f"tables/{target}/features_HPS9999_CTRL_{stain_type}.csv"
    elif target=="features_ScoreCAM":
        loadpath_RETT = f"tables/{target}/features_{rett_type}_RETT_{stain_type}_Resnet10_noavg_ScoreCAM.csv"
        loadpath_CTRL = f"tables/{target}/features_{rett_type}_Ctrl_{stain_type}_Resnet10_noavg_ScoreCAM.csv"
    else:
        print(f"Load Failed, can not find {loadpath}")
    # å®šä¹‰ä½ æƒ³è¦è¯»å–çš„åˆ—çš„ç´¢å¼•ï¼Œæ³¨æ„ Python ç´¢å¼•ä» 0 å¼€å§‹
    columns_to_use = [10] + list(range(12, 19)) + list(range(20, 22)) + list(range(36, 99))
    # è¯»å– CSV æ–‡ä»¶æ—¶ä»…åŠ è½½æŒ‡å®šçš„åˆ—
    df_RETT = pd.read_csv(loadpath_RETT, usecols=columns_to_use).dropna()  # åˆ é™¤åŒ…å« NaN çš„æ ·æœ¬
    df_CTRL = pd.read_csv(loadpath_CTRL, usecols=columns_to_use).dropna()  # åˆ é™¤åŒ…å« NaN çš„æ ·æœ¬
    print(f"ğŸ¦  LOAD {loadpath_RETT} {df_RETT.shape}")
    print(f"ğŸ§« LOAD {loadpath_CTRL} {df_CTRL.shape}")
    
#     # åˆ é™¤ç¦»ç¾¤ç‚¹
#     df_RETT_filtered = remove_outliers(df_RETT_scaled, multiplier=2)
#     df_CTRL_filtered = remove_outliers(df_CTRL_scaled, multiplier=2)
#     print(f"remove outliers {df_RETT_filtered.shape}, {df_CTRL_filtered.shape}")

#   æ·»åŠ çŠ¶æ€æ ‡ç­¾
    df_RETT['State'] = 'RETT'
    df_CTRL['State'] = 'CTRL'
    
    # åˆå¹¶æ•°æ®
    df_combined = pd.concat([df_CTRL, df_RETT])
    
    return df_combined, df_RETT, df_CTRL

def loadcsv_Standard(target, stain_type, rett_type):
    if target=="features_image":
        loadpath_RETT = f"tables/{target}/features_{rett_type}_RETT_{stain_type}.csv"
        loadpath_CTRL = f"tables/{target}/features_HPS9999_CTRL_{stain_type}.csv"
    elif target=="features_ScoreCAM":
        loadpath_RETT = f"tables/{target}/features_{rett_type}_RETT_{stain_type}_Resnet10_noavg_ScoreCAM.csv"
        loadpath_CTRL = f"tables/{target}/features_{rett_type}_Ctrl_{stain_type}_Resnet10_noavg_ScoreCAM.csv"
    else:
        print(f"Load Failed, can not find {loadpath}")
        
    # å®šä¹‰ä½ æƒ³è¦è¯»å–çš„åˆ—çš„ç´¢å¼•ï¼Œæ³¨æ„ Python ç´¢å¼•ä» 0 å¼€å§‹
    columns_to_use = [10] + list(range(12, 19)) + list(range(20, 22)) + list(range(36, 99))
    # è¯»å– CSV æ–‡ä»¶æ—¶ä»…åŠ è½½æŒ‡å®šçš„åˆ—
    df_RETT = pd.read_csv(loadpath_RETT, usecols=columns_to_use).dropna()  # åˆ é™¤åŒ…å« NaN çš„æ ·æœ¬
    df_CTRL = pd.read_csv(loadpath_CTRL, usecols=columns_to_use).dropna()  # åˆ é™¤åŒ…å« NaN çš„æ ·æœ¬
    print(f"ğŸ¦  LOAD {loadpath_RETT} {df_RETT.shape}")
    print(f"ğŸ§« LOAD {loadpath_CTRL} {df_CTRL.shape}")
    
    # æ ‡å‡†åŒ–æ•°æ®
    scaler = StandardScaler()
    df_RETT_scaled = scaler.fit_transform(df_RETT)
    df_CTRL_scaled = scaler.fit_transform(df_CTRL)
    # å°†æ ‡å‡†åŒ–åçš„æ•°æ®è½¬æ¢å› DataFrame
    df_RETT_scaled = pd.DataFrame(df_RETT_scaled, columns=df_RETT.columns)
    df_CTRL_scaled = pd.DataFrame(df_CTRL_scaled, columns=df_CTRL.columns)

    # æ·»åŠ çŠ¶æ€æ ‡ç­¾
    df_RETT = df_RETT_scaled.copy()
    df_CTRL = df_CTRL_scaled.copy()
    df_RETT['State'] = 'RETT'
    df_CTRL['State'] = 'CTRL'
    
    # åˆå¹¶æ•°æ®
    df_combined = pd.concat([df_CTRL, df_RETT])


    # åˆå¹¶æ•°æ®
    df_combined = pd.concat([df_CTRL, df_RETT])
    
    return df_combined, df_RETT, df_CTRL


def correlation_matrix(df, titles, savename):
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    correlation_matrix = df.corr()

    # è®¾ç½®å›¾çš„å°ºå¯¸
    plt.figure(figsize=(20, 16))

    # ä½¿ç”¨Seabornç»˜åˆ¶çƒ­å›¾
    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')

    # æ·»åŠ æ ‡é¢˜
    plt.title(f'Correlation Matrix of Features - {titles}')
#     plt.savefig(f'tables/{target}/{savename}_correlation_matrix.png', dpi=300)
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()



def validate_pca(df_combined, df_RETT, df_CTRL, loadpath, savename):
    print("ğŸ“Š PCA")
    # æå–ç‰¹å¾æ•°æ®
    features = df_combined.drop('State', axis=1)

    # åº”ç”¨ PCA
    pca = PCA(n_components=2)
    principalComponents = pca.fit_transform(features)
    principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])

    # è®¡ç®—è´¡çŒ®ç‡
    explained_variance_ratio = pca.explained_variance_ratio_
    print("Explained variance ratio:", explained_variance_ratio)

    # è·å–åŠ è½½çŸ©é˜µ
    loading_matrix = pca.components_.T
    loading_df = pd.DataFrame(loading_matrix, columns=['PC1', 'PC2'], index=features.columns)

    # å¯è§†åŒ–åŠ è½½çŸ©é˜µ
    plt.figure(figsize=(16, 12))
    sns.heatmap(loading_df, annot=True, cmap='coolwarm')
    plt.title('PCA Loading Matrix')
    plt.savefig(f'tables/{target}/{savename}_PCA_Matrix.png', dpi=300)
    plt.show()

    # é‡ç½®ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
    state_df = df_combined[['State']].reset_index(drop=True)
    finalDf = pd.concat([principalDf, state_df], axis=1)

    # ä½¿ç”¨ Seaborn ç»˜åˆ¶ PCA ç»“æœå›¾
    sns.scatterplot(data=finalDf, x='principal component 1', y='principal component 2', hue='State')
    plt.title('PCA of Dataset by State')
    plt.savefig(f'tables/{target}/{savename}_PCA.png', dpi=300)
    plt.show()
    
    


def validate_ttest(df_combined, df_RETT, df_CTRL, target, savename):
    print("ğŸ“Š ttest")
    # åˆå§‹åŒ–å­˜å‚¨ p å€¼çš„åˆ—è¡¨
    p_values = []

    # è¿›è¡Œ t-æ£€éªŒ
    for column in df_CTRL.columns[:-1]:  # å¿½ç•¥ 'State' åˆ—
        t_stat, p_val = ttest_ind(df_CTRL[column], df_RETT[column], equal_var=False)  # å¯ä»¥å‡è®¾ä¸ç­‰æ–¹å·®
        p_values.append((column, p_val))

    # å°† p å€¼è½¬åŒ–ä¸º DataFrame
    p_values_df = pd.DataFrame(p_values, columns=['Feature', 'p_value'])

    # æå–åŸå§‹ p å€¼åˆ—è¡¨
    p_values_list = p_values_df['p_value'].tolist()

    # è¿›è¡Œæ ¡æ­£
    rej, pval_corr = smm.multipletests(p_values_list, alpha=0.05, method='fdr_bh')[:2]

    # å°†æ ¡æ­£åçš„ p å€¼æ·»åŠ å› DataFrame
    p_values_df['p_corrected'] = pval_corr

    # ç­›é€‰æ˜¾è‘—ç‰¹å¾ï¼ˆä¾‹å¦‚æ ¡æ­£å p < 0.02ï¼‰
    significant_features = p_values_df[p_values_df['p_corrected'] < 0.02]

    # æŒ‰ p å€¼æ’åº
    significant_features = significant_features.sort_values(by='p_corrected')
    significant_features = pd.DataFrame(significant_features)
    
    # ä¿å­˜csv
    savepath = f"tables/{target}/{savename}_ttest.csv"
    significant_features.to_csv(savepath, index=False)
    print(f"Saved significant features to {savepath}")
    
#     # æ‰“å° DataFrame
#     print("significant_features: ", len(significant_features))
#     print(significant_features.to_string(index=False))

#     # å¯è§†åŒ–æ˜¾è‘—ç‰¹å¾çš„ p å€¼
#     plt.figure(figsize=(16, 10))
#     sns.barplot(x='p_corrected', y='Feature', data=significant_features, palette='viridis')
#     plt.title('Significant Features Differentiating CTRL and RETT')
#     plt.xlabel('p_value_corrected')
#     plt.ylabel('Features')
#     plt.axvline(x=0.02, color='r', linestyle='--')
#     plt.savefig(f'tables/{target}/{savename}_ttest.png', dpi=300)
#     plt.show()
    
    return significant_features
    


def validate_mannwhitneyutest(df_combined, df_RETT, df_CTRL, target, savename):
    print("ğŸ“Š Mann-Whitney U test")
    p_values = []
    # è¿›è¡Œ u-æ£€éªŒ
    for column in df_CTRL.columns[:-1]:
        stat, p_val = mannwhitneyu(df_CTRL[column], df_RETT[column], alternative='two-sided')
        p_values.append((column, p_val))
    
    # å°† p å€¼è½¬åŒ–ä¸º DataFrame
    p_values_df = pd.DataFrame(p_values, columns=['Feature', 'p_value'])
    
    # æå–åŸå§‹ p å€¼åˆ—è¡¨
    p_values_list = p_values_df['p_value'].tolist()
    
    # æå–åŸå§‹ p å€¼åˆ—è¡¨
    rej, pval_corr = smm.multipletests(p_values_list, alpha=0.05, method='fdr_bh')[:2]
    
    # å°†æ ¡æ­£åçš„ p å€¼æ·»åŠ å› DataFrame
    p_values_df['p_corrected'] = pval_corr
    
    # ç­›é€‰æ˜¾è‘—ç‰¹å¾ï¼ˆä¾‹å¦‚æ ¡æ­£å p < 0.02ï¼‰
    significant_features = p_values_df[p_values_df['p_corrected'] < 0.02]
    
    # æŒ‰ p å€¼æ’åº
    significant_features = significant_features.sort_values(by='p_corrected')
    significant_features = pd.DataFrame(significant_features)

    # ä¿å­˜csv
    savepath = f"tables/{target}/{savename}_utest.csv"
    significant_features.to_csv(savepath, index=False)
    print(f"Saved significant features to {savepath}")
    
#     # æ‰“å° DataFrame
#     print("significant_features: ", len(significant_features))
#     print(significant_features.to_string(index=False))

#     # å¯è§†åŒ–æ˜¾è‘—ç‰¹å¾çš„ p å€¼
#     plt.figure(figsize=(16, 10))
#     sns.barplot(x='p_corrected', y='Feature', data=significant_features, palette='viridis')
#     plt.title('Significant Features Differentiating CTRL and RETT')
#     plt.xlabel('p_value_corrected')
#     plt.ylabel('Features')
#     plt.axvline(x=0.02, color='r', linestyle='--')
#     plt.savefig(f'tables/{target}/{savename}_mannwhitneyutest.png', dpi=300)
#     plt.show()
    
    return significant_features
    
    
    
def validata_boxplot(data_all, target, rett_type, feature):
    # å‡è®¾ data_all æ˜¯ä¹‹å‰æ•´ç†å¥½çš„ DataFrame
    unique_stains = data_all['Stain_Type'].unique()  # è·å–æ‰€æœ‰æŸ“è‰²ç±»å‹
    p_values = []
    # è®¡ç®—æ¯ç§æŸ“è‰²ç±»å‹çš„ p å€¼
    for stain in unique_stains:
        group_ctrl = data_all[(data_all['State'] == 'CTRL') & (data_all['Stain_Type'] == stain)][feature]
        group_rett = data_all[(data_all['State'] == 'RETT') & (data_all['Stain_Type'] == stain)][feature]
        _, p_val = ttest_ind(group_ctrl, group_rett)
        p_values.append(p_val)
    for i in range(len(unique_stains)):
        print(f"p-value {unique_stains[i]}: {p_values[i]}")

    # è®¾ç½®é¢œè‰²
    palette_colors = {"CTRL": sns.color_palette(palette='bwr')[0], 
                      "RETT": sns.color_palette(palette='Pastel1')[0]}  # CTRL ä½¿ç”¨ç»¿è‰²ï¼ŒRETT ä½¿ç”¨ç´«è‰²
    savepath = f'tables/{target}/{target}_{rett_type}_{feature}.png'
    # ç»˜åˆ¶ç®±å‹å›¾
    plt.figure(figsize=(12, 8))
    sns.boxplot(x='Stain_Type', y=feature, hue='State', data=data_all, palette=palette_colors)
    plt.title(f'{feature} with Stains and Cell States')
    plt.ylabel(f'{feature}')
    plt.xlabel('Stain Type')
    plt.legend(title='Cell State')
    plt.savefig(savepath, dpi=300)
    plt.show()
    print(f"Saved BOX plot to {savepath}")

from skimage import feature, transform
from skimage.filters import threshold_otsu
from scipy.ndimage import distance_transform_edt
from skimage.segmentation import watershed

def is_close(point, other_points, threshold=5):
    """æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç‚¹åœ¨é˜ˆå€¼èŒƒå›´å†…"""
    for other in other_points:
        if np.linalg.norm(np.array(point) - np.array(other)) <= threshold:
            return True
    return False

def filter_contours_by_proximity(contours, nuclear_contours, proximity=5):
    """è¿‡æ»¤æ‰é è¿‘æ ¸å¿ƒè½®å»“çš„è½®å»“"""
    new_contours = []
    # å±•å¹³ nuclear_contours ä¸­çš„æ‰€æœ‰ç‚¹
    all_nuclear_points = [point for contour in nuclear_contours for point in contour]

    for contour in contours:
        # æ£€æŸ¥è½®å»“ä¸­çš„ä»»ä½•ç‚¹æ˜¯å¦é è¿‘æ ¸å¿ƒè½®å»“çš„ç‚¹
        if not any(is_close(point, all_nuclear_points, proximity) for point in contour):
            new_contours.append(contour)

    return new_contours

def compute_largest_eigenvalue(image, sigma=1):
    nuclear = (image!=0).astype(np.uint8)
    nuclear_scaled = transform.rescale(nuclear, 48/50)
    nuclear_padded = np.pad(nuclear_scaled, pad_width=10, mode='constant', constant_values=0)

    # è®¡ç®—ç»“æ„å¼ é‡
    result = feature.structure_tensor(image, sigma=sigma, order='rc')
    # ä»ç»“æ„å¼ é‡ä¸­è·å–ç‰¹å¾å€¼
    eigenvalues = feature.structure_tensor_eigenvalues(result)
    # è¿”å›æ¯ä¸ªç‚¹çš„æœ€å¤§ç‰¹å¾å€¼
    return np.max(eigenvalues, axis=0)*nuclear_padded

# ä½¿ç”¨distance_transform_edt
def apply_h_watershed(image, min_distance=5):
    mask = image > threshold_otsu(image)
    # è®¡ç®—è·ç¦»å˜æ¢
    distance = distance_transform_edt(mask)
    # åœ¨è·ç¦»å›¾ä¸­æ‰¾åˆ°å³°å€¼
    local_maxi = feature.peak_local_max(distance, min_distance=min_distance, labels=mask)
    # å°†å³°å€¼çš„åæ ‡è½¬æ¢ä¸ºæ ‡è®°çŸ©é˜µ
    if len(local_maxi)<=255:
        markers = np.zeros_like(image, dtype=np.uint8)
    else:
#         print("len(local_maxi)>255")
        markers = np.zeros_like(image, dtype=np.int32)
    for i, (row, col) in enumerate(local_maxi):
        markers[row, col] = i + 1
    # æ‰§è¡Œåˆ†æ°´å²­åˆ†å‰²
    labels_ws = watershed(-distance, markers, mask=mask)
    return labels_ws


from skimage import measure

def calculate_quantitative_metrics(nucleus_image, cc_mask):
    """
    è®¡ç®—ç»†èƒæ ¸å›¾åƒçš„é‡åŒ–æŒ‡æ ‡ã€‚
    
    å‚æ•°:
    nucleus_image: numpy.ndarray, ç»†èƒæ ¸å›¾åƒï¼Œç°åº¦å›¾
    cc_mask: numpy.ndarray, æŸ“è‰²ä¸­å¿ƒçš„æ©è†œï¼ŒäºŒå€¼å›¾
    
    è¿”å›:
    metrics: dict, åŒ…å«æ‰€æœ‰é‡åŒ–æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {}
    
    # è®¡ç®—å¯è§æŸ“è‰²ä¸­å¿ƒçš„æ•°é‡
    cc_labels = measure.label(cc_mask, connectivity=2)
    num_cc = np.max(cc_labels)
    metrics['chromatin_num'] = num_cc
    
    # è®¡ç®—ç»†èƒæ ¸é¢ç§¯
    nuclear_area = np.sum(nucleus_image > 0)
    metrics['nuclear_area'] = nuclear_area
    
    # è®¡ç®—å¹³å‡chromatiné¢ç§¯ (CA)
    cc_areas = [np.sum(cc_labels == i) for i in range(1, num_cc + 1)]
#     metrics['relative_cc_areas'] = relative_cc_areas
    metrics['chromatin_area'] = np.mean(cc_areas)

    # è®¡ç®—ç›¸å¯¹(æ ¸)chromatiné¢ç§¯å’Œ (RCA-S)
    metrics['RCA-S'] = np.sum(cc_areas)/nuclear_area

    # è®¡ç®—ç›¸å¯¹(æ ¸)chromatiné¢ç§¯å¹³å‡ (RCA-M)
    metrics['RCA-M'] = np.mean(cc_areas)/nuclear_area
    
    # è®¡ç®—ç»†èƒæ ¸å¼ºåº¦å¹³å‡
    nuclear_intensity = np.mean(nucleus_image[nucleus_image > 0])
    metrics['nuclear_intensity'] = nuclear_intensity

    # è®¡ç®—å¹³å‡chromatinå¼ºåº¦å¹³å‡ (CI-M)
    cc_intensities = [np.mean(nucleus_image[cc_labels == i]) for i in range(1, num_cc + 1)]
    metrics['chromatin_intensity'] = np.mean(cc_intensities)

    # è®¡ç®—ç›¸å¯¹(æ ¸)chromatinå¼ºåº¦å’Œ (RCI-S)
    metrics['RCI-S'] = np.sum(cc_intensities)/nuclear_intensity

    # è®¡ç®—ç›¸å¯¹(æ ¸)chromatinå¼ºåº¦å¹³å‡ (RCI-M)
    metrics['RCI-M'] = np.mean(cc_intensities)/nuclear_intensity
    
    # # è®¡ç®—ç›¸å¯¹(æ ¸)chromatinæ¯”ä¾‹ (RHF)
    # rhf = hf * rhi
    # metrics['relative_heterochromatin_fraction'] = rhf
    
    return metrics
 